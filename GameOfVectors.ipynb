{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"No Regrets\"\n",
    "## A word vector analysis of Game of Thrones\n",
    "following Siraj's example following Yuriy Guts' example\n",
    "\n",
    "### Generating and analyzing word vectors from novels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########## IMPORT DEPENDENCIES ##########\n",
    "# allows to use both Python 2 and 3 syntax\n",
    "from __future__ import absolute_import, division, print_function\n",
    "# for word encoding we\n",
    "import codecs\n",
    "# and of course for any NLP we'll need regex:\n",
    "import re\n",
    "# but seems we'll also be using a different library for this\n",
    "import glob\n",
    "# concurrency, in order to process the data more efficiently\n",
    "import multiprocessing\n",
    "# dealing with the OS, like reading a file\n",
    "import os\n",
    "# for niceness\n",
    "from pprint import pprint\n",
    "# for any sorts of NLP analysis\n",
    "import nltk\n",
    "# AAAAAND: WORD 2 VEC!! (the star of the show)\n",
    "# a google trained a NN on a huge set of word vectors;\n",
    "# it's a generalized collection of word vectors (=tha shit!)\n",
    "import gensim.models.word2vec as w2v\n",
    "# for dimensionality reduction for our puny brains\n",
    "# bc the wv are gonna be 300+ dimensions\n",
    "import sklearn.manifold\n",
    "# for math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# and plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########## CLEAN DATA ##########\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/got1.txt', 'data/got2.txt', 'data/got3.txt', 'data/got4.txt', 'data/got5.txt']\n"
     ]
    }
   ],
   "source": [
    "book_filenames = sorted(glob.glob(r'data/*.txt'))\n",
    "print(book_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making one large opus (that's what it is anyways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leisurely skimming 'data/got1.txt'...\n",
      "Read 1770659 characters of text, found ~250 plot characters...\n",
      "\n",
      "Leisurely skimming 'data/got2.txt'...\n",
      "Read 4071041 characters of text, found ~200 plot characters...\n",
      "\n",
      "Leisurely skimming 'data/got3.txt'...\n",
      "Read 6391405 characters of text, found ~150 plot characters...\n",
      "\n",
      "Leisurely skimming 'data/got4.txt'...\n",
      "Read 8107945 characters of text, found ~100 plot characters...\n",
      "\n",
      "Leisurely skimming 'data/got5.txt'...\n",
      "Read 9719485 characters of text, found ~50 plot characters...\n",
      "\n",
      "Whoa!... Okay peepz. Spoiler Alert! ...\n"
     ]
    }
   ],
   "source": [
    "# using UTF-8 as the common format for all texts\n",
    "corpus_raw = u\"\"\n",
    "# adding a fake count for amount of people in the books\n",
    "# just because GoT and there are so many deaths. ;)\n",
    "fun_count = 300\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Leisurely skimming '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book:\n",
    "        corpus_raw += book.read()\n",
    "    fun_count -= 50\n",
    "    print(\"Read {0} characters of text, found ~{1} plot characters...\".format(len(corpus_raw), fun_count))\n",
    "    print()\n",
    "print(\"Whoa!... Okay peepz. Spoiler Alert! ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sents = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    \"\"\"replace all non-alpha chars with a space, then split the text.\"\"\"\n",
    "    # tr -sc 'a-zA-Z' ' ' < GoT_corpus.txt ðŸ˜‰\n",
    "    clean = re.sub(r\"[^a-zA-Z]\", \" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# awesome stuff with list comprehension\n",
    "# tokenize each word in each sentence\n",
    "sentences = [sentence_to_wordlist(s) for s in raw_sents if len(raw_sents) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their droppings speckled the gargoyles that rose twelve feet tall on either side of him, a hellhound and a wyvern, two of the thousand that brooded over the walls of the ancient fortress.\n",
      "['Their', 'droppings', 'speckled', 'the', 'gargoyles', 'that', 'rose', 'twelve', 'feet', 'tall', 'on', 'either', 'side', 'of', 'him', 'a', 'hellhound', 'and', 'a', 'wyvern', 'two', 'of', 'the', 'thousand', 'that', 'brooded', 'over', 'the', 'walls', 'of', 'the', 'ancient', 'fortress']\n"
     ]
    }
   ],
   "source": [
    "sent_num = 11\n",
    "\n",
    "print(raw_sents[sent_num])\n",
    "print(sentence_to_wordlist(raw_sents[sent_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the amount of word tokens in the corpus\n",
    "token_count = sum([len(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,818,103 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have vectors, we have three main tasks we want to perform:\n",
    "- DISTANCE\n",
    "- SIMILARITY\n",
    "- RANKING\n",
    "\n",
    "(e.g. scan all scientific papers and figure out which has the highest ranking regarding having the most information about climate change! Exciting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the dimensionality of the word vectors.\n",
    "# higher dimensionality makes them more computationally expensive to train\n",
    "# but also more accurate\n",
    "# GENERALLY: more dimensions = more generalized\n",
    "num_features = 300\n",
    "\n",
    "# smallest set that we want to recognize when converting to a vector\n",
    "min_word_count = 3\n",
    "\n",
    "# number of threads to run in parallel\n",
    "# the more workers we have, the faster we can train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# context window size (how many words looked at in one peek)\n",
    "context_window = 7\n",
    "\n",
    "# downsample setting for frequent words\n",
    "# 0 - 1e-5 is good for this\n",
    "# essentially it means that we don't want to look at words that appear too frequently too frequently\n",
    "# how often do we look at a word - the more frequent, the less often we want to look at it\n",
    "downsampling = 1e-3\n",
    "\n",
    "# seed for the random number generator (RNG)\n",
    "# to make the results reproducible\n",
    "# the seed makes sure that it is deterministic - which is useful for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using gensim library for creating the model\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg = 1,\n",
    "    seed = seed,\n",
    "    workers = num_workers,\n",
    "    size = num_features,\n",
    "    min_count = min_word_count,\n",
    "    window = context_window,\n",
    "    sample = downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building the model\n",
    "thrones2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 17277\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length: {}\".format(len(thrones2vec.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7023185"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving it to a file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a folder for the files\n",
    "if not os.path.exists(\"trainer\"):\n",
    "    os.makedirs(\"trained\")\n",
    "    \n",
    "# save the output of the trained model to a file\n",
    "thrones2vec.save(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these are 300-dimensional word vectors, we'll need to apply some dimensionality reduction in order to be able to plot them into 2-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing into 2D space for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using t-SNE (t-distributed Stochastic Neighbor Mapping)\n",
    "# for squashing the dimensions of our data in order to be able to visualize it\n",
    "# here we choose a 2D representation\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assigning a variable to the big matrix holding all the word vectors\n",
    "# 'syn0' is the feature matrix generated by gensim\n",
    "all_word_vec_matrix = thrones2vec.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training t-SNE\n",
    "Have to learn about why this is \"training\" here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vec_matrix_2D = tsne.fit_transform(all_word_vec_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:car]",
   "language": "python",
   "name": "conda-env-car-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
